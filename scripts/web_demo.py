"""
æ–‡ä»¶ç”¨é€”ï¼š
  ä½¿ç”¨ Streamlit æ„å»ºä¸€ä¸ªç®€æ´çš„ MiniMind èŠå¤©å‰ç«¯ï¼Œæ”¯æŒä¸¤ç§æ¨¡å‹æ¥æºï¼š
    1) æœ¬åœ°æ¨¡å‹ï¼ˆé€šè¿‡ Transformers ç›´æ¥åŠ è½½ï¼‰
    2) è¿œç«¯ APIï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰
  åŠŸèƒ½ç‚¹ï¼š
    - è‡ªå®šä¹‰æ ·å¼ï¼ˆåœ†å½¢æ“ä½œæŒ‰é’®ã€æ ‡é¢˜æ ç­‰ï¼‰
    - å¯¹è¯å†å²è®°å¿†ï¼ˆå¯é…ç½®æºå¸¦è½®æ•°ï¼‰
    - æµå¼è¾“å‡ºï¼ˆæœ¬åœ°/è¿œç«¯å‡æ”¯æŒï¼‰
    - â€œæ¨ç†å†…å®¹â€(<think>...</think>) æŠ˜å å±•ç¤º

"""

import random
import re
from threading import Thread

import torch
import numpy as np
import streamlit as st

# é¡µé¢åŸºç¡€è®¾ç½®
st.set_page_config(page_title="MiniMind", initial_sidebar_state="collapsed")

# é¡µé¢æ ·å¼ï¼šä¸»è¦ç”¨äºåœ†å½¢æŒ‰é’®ã€è¾¹è·å¾®è°ƒ
st.markdown("""
    <style>
        /* ä¾§æ æŒ‰é’®æ ·å¼ï¼ˆåœ†å½¢ã€å°å·ã€æµ…è‰²ï¼‰ */
        .stButton button {
            border-radius: 50% !important;
            width: 32px !important;
            height: 32px !important;
            padding: 0 !important;
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #666 !important;
            margin: 5px 10px 5px 0 !important;
        }
        .stButton button:hover {
            border-color: #999 !important;
            color: #333 !important;
            background-color: #f5f5f5 !important;
        }
        .stMainBlockContainer > div:first-child { margin-top: -50px !important; }
        .stApp > div:last-child { margin-bottom: -35px !important; }

        /* é‡ç½®æŒ‰é’®åŸºç¡€æ ·å¼ï¼ˆæ›´å°çš„åœ†å½¢ Ã— æŒ‰é’®ï¼‰ */
        .stButton > button {
            all: unset !important;
            box-sizing: border-box !important;
            border-radius: 50% !important;
            width: 18px !important;
            height: 18px !important;
            min-width: 18px !important;
            min-height: 18px !important;
            max-width: 18px !important;
            max-height: 18px !important;
            padding: 0 !important;
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #888 !important;
            cursor: pointer !important;
            transition: all 0.2s ease !important;
            margin: 0 2px !important;
        }
    </style>
""", unsafe_allow_html=True)

# ç³»ç»Ÿçº§æ¶ˆæ¯ï¼ˆå¯æ³¨å…¥ system æŒ‡ä»¤ï¼‰
system_prompt = []
# æ¨æ–­è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"


def process_assistant_content(content):
    """
    ç”¨é€”ï¼š
        å¤„ç†åŠ©æ‰‹è¾“å‡ºçš„æ–‡æœ¬ï¼Œå°† <think> ... </think> åŒ…è£¹çš„æ¨ç†è¿‡ç¨‹è½¬æ¢ä¸ºå¯æŠ˜å  HTMLã€‚
    é€»è¾‘ï¼š
        - ä»…å½“æ¨¡å‹ååŒ…å« 'R1' æ—¶å±•ç¤º think åŒºåŸŸï¼ˆæœ¬åœ°/è¿œç«¯éƒ½åšæ£€æµ‹ï¼‰
        - åŒæ—¶å…¼å®¹ï¼šåªæœ‰èµ·å§‹æˆ–åªæœ‰ç»“æŸæ ‡ç­¾çš„è¾¹ç•Œæƒ…å†µ
    """
    if model_source == "API" and 'R1' not in api_model_name:
        return content
    if model_source != "API" and 'R1' not in MODEL_PATHS[selected_model][1]:
        return content

    # å®Œæ•´ <think>...</think>
    if '<think>' in content and '</think>' in content:
        content = re.sub(
            r'(<think>)(.*?)(</think>)',
            r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\2</details>',
            content,
            flags=re.DOTALL
        )

    # åªæœ‰ <think> èµ·å§‹
    if '<think>' in content and '</think>' not in content:
        content = re.sub(
            r'<think>(.*?)$',
            r'<details open style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†ä¸­...</summary>\1</details>',
            content,
            flags=re.DOTALL
        )

    # åªæœ‰ </think> ç»“æŸ
    if '<think>' not in content and '</think>' in content:
        content = re.sub(
            r'(.*?)</think>',
            r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\1</details>',
            content,
            flags=re.DOTALL
        )

    return content


@st.cache_resource
def load_model_tokenizer(model_path):
    """
    ä½œç”¨ï¼š
        åŠ è½½æœ¬åœ° Transformers æ¨¡å‹ä¸åˆ†è¯å™¨ï¼Œå¹¶åœ¨é¦–æ¬¡è°ƒç”¨åè¿›è¡Œç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰ã€‚
    å‚æ•°ï¼š
        model_path: æ¨¡å‹ç›®å½•ï¼ˆåŒ…å« configã€æƒé‡ã€tokenizerï¼‰
    è¿”å›ï¼š
        (model.eval().to(device), tokenizer)
    """
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    model = model.eval().to(device)
    return model, tokenizer


def clear_chat_messages():
    """ æ¸…ç©ºä¼šè¯ç¼“å­˜ï¼ˆmessages & chat_messagesï¼‰ """
    del st.session_state.messages
    del st.session_state.chat_messages


def init_chat_messages():
    """
    åˆå§‹åŒ–å¹¶å›æ”¾å†å²æ¶ˆæ¯ï¼š
      - assistant æ¶ˆæ¯ä½¿ç”¨ chat_message UI æ¸²æŸ“å¹¶å¯å•æ¡åˆ é™¤
      - user æ¶ˆæ¯å³å¯¹é½ç°åº•æ°”æ³¡
    """
    if "messages" in st.session_state:
        for i, message in enumerate(st.session_state.messages):
            if message["role"] == "assistant":
                with st.chat_message("assistant", avatar=image_url):
                    st.markdown(process_assistant_content(message["content"]), unsafe_allow_html=True)
                    # å•æ¡åˆ é™¤ï¼ˆä¼šåˆ é™¤ä¸€å¯¹ Q/Aï¼‰
                    if st.button("ğŸ—‘", key=f"delete_{i}"):
                        st.session_state.messages.pop(i)
                        st.session_state.messages.pop(i - 1)
                        st.session_state.chat_messages.pop(i)
                        st.session_state.chat_messages.pop(i - 1)
                        st.rerun()
            else:
                st.markdown(
                    f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: #ddd; border-radius: 10px; color: black;">{message["content"]}</div></div>',
                    unsafe_allow_html=True)
    else:
        st.session_state.messages = []
        st.session_state.chat_messages = []

    return st.session_state.messages


def regenerate_answer(index):
    """ é‡æ–°ç”Ÿæˆï¼šåˆ é™¤æœ€åä¸€æ¡ Aï¼ˆå’Œå¯¹åº”çš„æ¸²æŸ“ï¼‰ï¼Œè§¦å‘é‡æ¸²æŸ“ """
    st.session_state.messages.pop()
    st.session_state.chat_messages.pop()
    st.rerun()


def delete_conversation(index):
    """ åˆ é™¤æŸä¸€è½®å¯¹è¯ï¼ˆQ/A å„ä¸€æ¡ï¼‰ """
    st.session_state.messages.pop(index)
    st.session_state.messages.pop(index - 1)
    st.session_state.chat_messages.pop(index)
    st.session_state.chat_messages.pop(index - 1)
    st.rerun()


# ========== ä¾§è¾¹æ ï¼šæ¨¡å‹ä¸æ¨ç†å‚æ•° ==========
st.sidebar.title("æ¨¡å‹è®¾å®šè°ƒæ•´")
# å†å²å¯¹è¯è½®æ•°ï¼ˆå¿…é¡»ä¸ºå¶æ•°ï¼ŒQ+A ä¸ºä¸€ç»„ï¼›0 è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰
st.session_state.history_chat_num = st.sidebar.slider("Number of Historical Dialogues", 0, 6, 0, step=2)
# æœ€å¤§æ–°ç”Ÿæˆ tokenï¼ˆç”¨äºé™åˆ¶ä¸Šä¸‹æ–‡ä¸ç”Ÿæˆé•¿åº¦ï¼‰
st.session_state.max_new_tokens = st.sidebar.slider("Max Sequence Length", 256, 8192, 8192, step=1)
# é‡‡æ ·æ¸©åº¦
st.session_state.temperature = st.sidebar.slider("Temperature", 0.6, 1.2, 0.85, step=0.01)

# æ¨¡å‹æ¥æºé€‰æ‹©ï¼šæœ¬åœ° / API
model_source = st.sidebar.radio("é€‰æ‹©æ¨¡å‹æ¥æº", ["æœ¬åœ°æ¨¡å‹", "API"], index=0)

if model_source == "API":
    # è¿œç«¯ API å‚æ•°ï¼ˆOpenAI å…¼å®¹ï¼‰
    api_url = st.sidebar.text_input("API URL", value="http://127.0.0.1:8000/v1")
    api_model_id = st.sidebar.text_input("Model ID", value="minimind")
    api_model_name = st.sidebar.text_input("Model Name", value="MiniMind2")
    api_key = st.sidebar.text_input("API Key", value="none", type="password")
    slogan = f"Hi, I'm {api_model_name}"
else:
    # æœ¬åœ°æ¨¡å‹é›†åˆï¼ˆåç§° -> [è·¯å¾„, å±•ç¤ºå]ï¼‰
    MODEL_PATHS = {
        "MiniMind2-R1 (0.1B)": ["../MiniMind2-R1", "MiniMind2-R1"],
        "MiniMind2-Small-R1 (0.02B)": ["../MiniMind2-Small-R1", "MiniMind2-Small-R1"],
        "MiniMind2 (0.1B)": ["../MiniMind2", "MiniMind2"],
        "MiniMind2-MoE (0.15B)": ["../MiniMind2-MoE", "MiniMind2-MoE"],
        "MiniMind2-Small (0.02B)": ["../MiniMind2-Small", "MiniMind2-Small"]
    }
    # é»˜è®¤ MiniMind2
    selected_model = st.sidebar.selectbox('Models', list(MODEL_PATHS.keys()), index=2)
    model_path = MODEL_PATHS[selected_model][0]
    slogan = f"Hi, I'm {MODEL_PATHS[selected_model][1]}"

# é¡¶éƒ¨æ ‡é¢˜ä¸ logo
image_url = "https://www.modelscope.cn/api/v1/studio/gongjy/MiniMind/repo?Revision=master&FilePath=images%2Flogo2.png&View=true"
st.markdown(
    f'<div style="display: flex; flex-direction: column; align-items: center; text-align: center; margin: 0; padding: 0;">'
    '<div style="font-style: italic; font-weight: 900; margin: 0; padding-top: 4px; display: flex; align-items: center; justify-content: center; flex-wrap: wrap; width: 100%;">'
    f'<img src="{image_url}" style="width: 45px; height: 45px; "> '
    f'<span style="font-size: 26px; margin-left: 10px;">{slogan}</span>'
    '</div>'
    '<span style="color: #bbb; font-style: italic; margin-top: 6px; margin-bottom: 10px;">å†…å®¹å®Œå…¨ç”±AIç”Ÿæˆï¼Œè¯·åŠ¡å¿…ä»”ç»†ç”„åˆ«<br>Content AI-generated, please discern with care</span>'
    '</div>',
    unsafe_allow_html=True
)


def setup_seed(seed):
    """
    å›ºå®šéšæœºæ€§ï¼Œä¾¿äºè°ƒè¯•ä¸å¤ç°ã€‚
    æ³¨æ„ï¼šå¼€å¯ deterministic=True ä¼šç¦ç”¨æŸäº› cuDNN åŠ é€Ÿã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    """
    é¡µé¢ä¸»é€»è¾‘ï¼š
      - åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆæˆ–å‡†å¤‡ API è°ƒç”¨å‚æ•°ï¼‰
      - å›æ”¾å†å²æ¶ˆæ¯
      - å¤„ç†æ–°è¾“å…¥ï¼ˆæœ¬åœ°/è¿œç«¯ä¸¤æ¡æ¨ç†è·¯å¾„ï¼Œå‡æ”¯æŒæµå¼æ¸²æŸ“ï¼‰
      - è®°å½•å¹¶æ¸²æŸ“æœ¬è½®é—®ç­”
    """
    # ä»…å½“é€‰æ‹©â€œæœ¬åœ°æ¨¡å‹â€æ—¶åŠ è½½ï¼›API æ¨¡å¼ä¸‹ç”±æœåŠ¡ç«¯æ¨ç†
    if model_source == "æœ¬åœ°æ¨¡å‹":
        model, tokenizer = load_model_tokenizer(model_path)
    else:
        model, tokenizer = None, None

    # åˆå§‹åŒ–çŠ¶æ€å­˜å‚¨
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.chat_messages = []

    messages = st.session_state.messages

    # å›æ”¾å†å²æ¶ˆæ¯ï¼ˆåŒ…å«åˆ é™¤æŒ‰é’®ï¼‰
    for i, message in enumerate(messages):
        if message["role"] == "assistant":
            with st.chat_message("assistant", avatar=image_url):
                st.markdown(process_assistant_content(message["content"]), unsafe_allow_html=True)
                # åˆ é™¤å½“å‰è¿™è½®ï¼ˆQ/A ä¸¤æ¡ï¼‰
                if st.button("Ã—", key=f"delete_{i}"):
                    st.session_state.messages = st.session_state.messages[:i - 1]
                    st.session_state.chat_messages = st.session_state.chat_messages[:i - 1]
                    st.rerun()
        else:
            # å³å¯¹é½æ°”æ³¡æ¸²æŸ“ç”¨æˆ·æ¶ˆæ¯
            st.markdown(
                f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: gray; border-radius: 10px; color:white; ">{message["content"]}</div></div>',
                unsafe_allow_html=True)

    # è¾“å…¥æ¡†
    prompt = st.chat_input(key="input", placeholder="ç»™ MiniMind å‘é€æ¶ˆæ¯")

    # â€œé‡æ–°ç”Ÿæˆâ€åœºæ™¯ï¼šç”¨æœ€åä¸€æ¬¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºå½“å‰ prompt
    if hasattr(st.session_state, 'regenerate') and st.session_state.regenerate:
        prompt = st.session_state.last_user_message
        regenerate_index = st.session_state.regenerate_index
        delattr(st.session_state, 'regenerate')
        delattr(st.session_state, 'last_user_message')
        delattr(st.session_state, 'regenerate_index')

    if prompt:
        # ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼ˆå³å¯¹é½ï¼‰
        st.markdown(
            f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: gray; border-radius: 10px; color:white; ">{prompt}</div></div>',
            unsafe_allow_html=True)

        # è®°å½•æ¶ˆæ¯ï¼ˆè£å‰ªé•¿åº¦ï¼Œé¿å…è¶…é•¿ï¼‰
        messages.append({"role": "user", "content": prompt[-st.session_state.max_new_tokens:]})
        st.session_state.chat_messages.append({"role": "user", "content": prompt[-st.session_state.max_new_tokens:]})

        # åŠ©æ‰‹æ¶ˆæ¯å®¹å™¨ï¼ˆç”¨äºæµå¼æ›´æ–°ï¼‰
        with st.chat_message("assistant", avatar=image_url):
            placeholder = st.empty()

            if model_source == "API":
                # ===== è¿œç«¯ API æ¨ç†ï¼ˆOpenAI å…¼å®¹ï¼‰=====
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=api_key, base_url=api_url)

                    # å†å²å¯¹è¯æ¡æ•°ï¼ˆ+1 åŒ…å«å½“å‰è¿™æ¡ï¼‰
                    history_num = st.session_state.history_chat_num + 1
                    conversation_history = system_prompt + st.session_state.chat_messages[-history_num:]

                    answer = ""
                    response = client.chat.completions.create(
                        model=api_model_id,
                        messages=conversation_history,
                        stream=True,
                        temperature=st.session_state.temperature
                    )
                    # æµå¼æ¸²æŸ“
                    for chunk in response:
                        content = chunk.choices[0].delta.content or ""
                        answer += content
                        placeholder.markdown(process_assistant_content(answer), unsafe_allow_html=True)

                except Exception as e:
                    answer = f"APIè°ƒç”¨å‡ºé”™: {str(e)}"
                    placeholder.markdown(answer, unsafe_allow_html=True)

            else:
                # ===== æœ¬åœ°æ¨¡å‹æ¨ç†ï¼ˆTransformers generate + TextIteratorStreamerï¼‰=====
                random_seed = random.randint(0, 2 ** 32 - 1)
                setup_seed(random_seed)

                # å¸¦å…¥ç³»ç»Ÿæç¤ºä¸æœ‰é™å†å²
                st.session_state.chat_messages = system_prompt + st.session_state.chat_messages[-(st.session_state.history_chat_num + 1):]

                # chat_template ç”Ÿæˆ prompt
                new_prompt = tokenizer.apply_chat_template(
                    st.session_state.chat_messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                # ç¼–ç ä¸ºå¼ é‡
                inputs = tokenizer(
                    new_prompt,
                    return_tensors="pt",
                    truncation=True
                ).to(device)

                # æµå¼è¾“å‡ºå™¨ï¼ˆé€å¢é‡ token æ¨é€ï¼‰
                streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
                generation_kwargs = {
                    "input_ids": inputs.input_ids,
                    "max_length": inputs.input_ids.shape[1] + st.session_state.max_new_tokens,
                    "num_return_sequences": 1,
                    "do_sample": True,
                    "attention_mask": inputs.attention_mask,
                    "pad_token_id": tokenizer.pad_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                    "temperature": st.session_state.temperature,
                    "top_p": 0.85,
                    "streamer": streamer,
                }

                # åå°çº¿ç¨‹ç”Ÿæˆ
                Thread(target=model.generate, kwargs=generation_kwargs).start()

                # å‰å°é€æ®µæ¸²æŸ“
                answer = ""
                for new_text in streamer:
                    answer += new_text
                    placeholder.markdown(process_assistant_content(answer), unsafe_allow_html=True)

            # è®°å½•åŠ©æ‰‹å›ç­”
            messages.append({"role": "assistant", "content": answer})
            st.session_state.chat_messages.append({"role": "assistant", "content": answer})

            # å½“å‰æ¡ç›®å°±åœ°æä¾›åˆ é™¤æŒ‰é’®ï¼ˆåˆ é™¤æœ€è¿‘ä¸€è½®ï¼‰
            with st.empty():
                if st.button("Ã—", key=f"delete_{len(messages) - 1}"):
                    st.session_state.messages = st.session_state.messages[:-2]
                    st.session_state.chat_messages = st.session_state.chat_messages[:-2]
                    st.rerun()


if __name__ == "__main__":
    # æ¨è¿Ÿå¯¼å…¥ä»¥åŠ å¿«é¦–æ¬¡é¡µé¢åŠ è½½
    from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
    main()
